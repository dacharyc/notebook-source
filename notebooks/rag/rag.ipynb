{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas Vector Search - Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a companion for the [Retrieval-Augmented Generation (RAG)](https://www.mongodb.com/docs/atlas/atlas-vector-search/rag/) page. Refer to the page for set up steps and explanation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --quiet pymongo langchain langchain_community langchain_mongodb langchain_huggingface pypdf sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"https://investors.mongodb.com/node/12236/pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Load the embedding model (https://huggingface.co/nomic-ai/nomic-embed-text-v1\")\n",
    "model = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\", model_kwargs={ \"trust_remote_code\": True })\n",
    "\n",
    "# Connect to your Atlas cluster\n",
    "client = MongoClient(\"<connection-string>\")\n",
    "collection = client[\"rag_db\"][\"test\"]\n",
    "\n",
    "# Store the data as vector embeddings in Atlas\n",
    "vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = model,\n",
    "    collection = collection,\n",
    "    index_name = \"vector_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "# Create your index model, then create the search index\n",
    "search_index_model = SearchIndexModel(\n",
    "  definition = {\n",
    "    \"fields\": [\n",
    "      {\n",
    "        \"type\": \"vector\",\n",
    "        \"numDimensions\": 768,\n",
    "        \"path\": \"embedding\",\n",
    "        \"similarity\": \"cosine\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  name = \"vector_index\",\n",
    "  type = \"vectorSearch\"\n",
    ")\n",
    "collection.create_search_index(model=search_index_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Atlas Vector Search as a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "   search_type = \"similarity\"\n",
    ")\n",
    "\n",
    "# Run a sample query in order of relevance\n",
    "retriever.invoke(\"AI technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "# Authenticate to your Hugging Face account\n",
    "os.environ[\"HF_TOKEN\"] = \"<token>\"\n",
    "\n",
    "# Access the LLM (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Create prompt and RAG workflow\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "   Answer the following question based on the given context.\n",
    "\n",
    "   Question: {question}\n",
    "   Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "   { \"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "   | prompt\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Prompt the LLM\n",
    "question = \"In a few sentences, what are MongoDB's latest AI announcements?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
